{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARQUITETURA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) RAJPURKAR, Pranav et. al. Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas básicas\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Biblioteca para rede neural\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Flatten, Dense, Activation, ReLU, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('dados.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando os conjuntos de treinamento e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjunto de treinamento\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "#Conjunto de teste\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tDIMENSÕES \n",
      "-------------------------\n",
      "X_train = (18358, 1000, 12)\n",
      "y_train = (18358, 5)\n",
      "-------------------------\n",
      "X_test = (2055, 1000, 12)\n",
      "y_test = (2055, 5)\n",
      "-------------------------\n",
      "\tSUMÁRIO \n",
      "-------------------------\n",
      "Total de exemplos de treinamento = 18358\n",
      "Total de exemplos de teste = 2055\n",
      "Total de amostras = 1000\n"
     ]
    }
   ],
   "source": [
    "#Sumário\n",
    "print(\"\\tDIMENSÕES \\n-------------------------\")\n",
    "print(f'X_train = {X_train.shape}')\n",
    "print(f'y_train = {y_train.shape}')\n",
    "print(\"-------------------------\")\n",
    "print(f'X_test = {X_test.shape}')\n",
    "print(f'y_test = {y_test.shape}')\n",
    "print(\"-------------------------\")\n",
    "print(\"\\tSUMÁRIO \\n-------------------------\")\n",
    "print(f\"Total de exemplos de treinamento = {X_train.shape[0]}\")\n",
    "print(f\"Total de exemplos de teste = {X_test.shape[0]}\")\n",
    "print(f\"Total de amostras = {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape = (1000, 12)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(1000, 12)\n",
    "print(f\"Input shape = {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESENVOLVIMENTO DA REDE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_drop = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = 'he_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Camada de entrada\n",
    "input_1 = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro bloco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 64\n",
    "\n",
    "conv_1 = Conv1D(kernel_size=16, strides=1, filters=num_filter,padding=\"same\",kernel_initializer=initializer)(input_1)\n",
    "bn_1 = BatchNormalization()(conv_1)\n",
    "relu_1 = ReLU()(bn_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "conv_2 = Conv1D(kernel_size=16,filters=num_filter,strides=1,padding=\"same\",kernel_initializer=initializer)(relu_1)\n",
    "bn_2 = BatchNormalization()(conv_2)\n",
    "relu_2 = ReLU()(bn_2)\n",
    "drop_1 = Dropout(rate_drop)(relu_2)\n",
    "conv_3 = Conv1D(kernel_size = 16, filters=num_filter,strides=2,padding=\"same\",kernel_initializer=initializer)(drop_1)\n",
    "\n",
    "#Short connection\n",
    "short_1 = MaxPooling1D(pool_size=1, strides=2)(relu_1)\n",
    "\n",
    "#Adicionando camadas\n",
    "add_1 = Add()([conv_3, short_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_3 = BatchNormalization()(add_1)\n",
    "relu_3 = ReLU()(bn_3)\n",
    "drop_2 = Dropout(rate_drop)(relu_3)\n",
    "conv_4 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_2)\n",
    "bn_4 = BatchNormalization()(conv_4)\n",
    "relu_4 = ReLU()(bn_4)\n",
    "drop_3 = Dropout(rate_drop)(relu_4)\n",
    "conv_5 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_3)\n",
    "\n",
    "#Short connection\n",
    "short_2 = MaxPooling1D(pool_size=1, strides=1)(add_1)\n",
    "\n",
    "#Adicionando camadas\n",
    "add_2 = Add()([conv_5, short_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terceiro bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_5 = BatchNormalization()(add_2)\n",
    "relu_5 = ReLU()(bn_5)\n",
    "drop_4 = Dropout(rate_drop)(relu_5)\n",
    "conv_6 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_4)\n",
    "bn_6 = BatchNormalization()(conv_6)\n",
    "relu_6 = ReLU()(bn_6)\n",
    "drop_5 = Dropout(rate_drop)(relu_6)\n",
    "conv_7 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_5)\n",
    "\n",
    "#Short connection\n",
    "short_3 = MaxPooling1D(pool_size = 1, strides=2)(add_2)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_3 = Add()([conv_7, short_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quarto bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_7 = BatchNormalization()(add_3)\n",
    "relu_7 = ReLU()(bn_7)\n",
    "drop_6 = Dropout(rate_drop)(relu_7)\n",
    "conv_8 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_6)\n",
    "bn_8 = BatchNormalization()(conv_8)\n",
    "relu_8 = ReLU()(bn_8)\n",
    "drop_7 = Dropout(rate_drop)(relu_8)\n",
    "conv_9 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_7)\n",
    "\n",
    "#Short connection\n",
    "short_4 = MaxPooling1D(pool_size = 1, strides=1)(add_3)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_4 = Add()([conv_9, short_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 128\n",
    "\n",
    "#Bloco residual\n",
    "bn_9 = BatchNormalization()(add_4)\n",
    "relu_9 = ReLU()(bn_9)\n",
    "drop_8 = Dropout(rate_drop)(relu_9)\n",
    "conv_10 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_8)\n",
    "bn_10 = BatchNormalization()(conv_10)\n",
    "relu_10 = ReLU()(bn_10)\n",
    "drop_9 = Dropout(rate_drop)(relu_10)\n",
    "conv_11 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_9)\n",
    "\n",
    "#Short connection\n",
    "conv_aj5 = Conv1D(kernel_size=16,filters=num_filter,strides=1,padding=\"same\")(add_4) #Ajustar o número de filtros para adicionar depois o Max ao conv_11\n",
    "short_5 = MaxPooling1D(pool_size = 1, strides=2)(conv_aj5)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_5 = Add()([conv_11, short_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sexto bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_11 = BatchNormalization()(add_5)\n",
    "relu_11 = ReLU()(bn_11)\n",
    "drop_9 = Dropout(rate_drop)(relu_11)\n",
    "conv_12 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_9)\n",
    "bn_12 = BatchNormalization()(conv_12)\n",
    "relu_12 = ReLU()(bn_12)\n",
    "drop_10 = Dropout(rate_drop)(relu_12)\n",
    "conv_13 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_10)\n",
    "\n",
    "#Short connection\n",
    "short_6 = MaxPooling1D(pool_size = 1, strides=1)(add_5)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_6 = Add()([conv_13, short_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sétimo bloco residual  \n",
    "Strides = 2  \n",
    "Filtros = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_13 = BatchNormalization()(add_6)\n",
    "relu_13 = ReLU()(bn_13)\n",
    "drop_11 = Dropout(rate_drop)(relu_13)\n",
    "conv_14 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_11)\n",
    "bn_14 = BatchNormalization()(conv_14)\n",
    "relu_14 = ReLU()(bn_14)\n",
    "drop_12 = Dropout(rate_drop)(relu_14)\n",
    "conv_15 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_12)\n",
    "\n",
    "#Short connection\n",
    "short_7 = MaxPooling1D(pool_size = 1, strides=2)(add_6)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_7 = Add()([conv_15, short_7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oitavo bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_15 = BatchNormalization()(add_7)\n",
    "relu_15 = ReLU()(bn_15)\n",
    "drop_13 = Dropout(rate_drop)(relu_15)\n",
    "conv_16 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_13)\n",
    "bn_16 = BatchNormalization()(conv_16)\n",
    "relu_16 = ReLU()(bn_16)\n",
    "drop_14 = Dropout(rate_drop)(relu_16)\n",
    "conv_17 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_14)\n",
    "\n",
    "#Short connection\n",
    "short_8 = MaxPooling1D(pool_size = 1, strides=1)(add_7)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_8 = Add()([conv_17, short_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nono bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 192\n",
    "\n",
    "#Bloco residual\n",
    "bn_17 = BatchNormalization()(add_8)\n",
    "relu_17 = ReLU()(bn_17)\n",
    "drop_15 = Dropout(rate_drop)(relu_17)\n",
    "conv_18 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_15)\n",
    "bn_18 = BatchNormalization()(conv_18)\n",
    "relu_18 = ReLU()(bn_18)\n",
    "drop_16 = Dropout(rate_drop)(relu_18)\n",
    "conv_19 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\")(drop_16)\n",
    "\n",
    "#Short connection\n",
    "conv_aj17 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\")(add_8)\n",
    "short_9 = MaxPooling1D(pool_size = 1, strides=2)(conv_aj17)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_9 = Add()([conv_19, short_9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_19 = BatchNormalization()(add_9)\n",
    "relu_19 = ReLU()(bn_19)\n",
    "drop_17 = Dropout(rate_drop)(relu_19)\n",
    "conv_20 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_17)\n",
    "bn_20 = BatchNormalization()(conv_20)\n",
    "relu_20 = ReLU()(bn_20)\n",
    "drop_18 = Dropout(rate_drop)(relu_20)\n",
    "conv_21 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_18)\n",
    "\n",
    "#Short connection\n",
    "short_10 = MaxPooling1D(pool_size = 1, strides=1)(add_9)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_10 = Add()([conv_21, short_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo primeiro bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_21 = BatchNormalization()(add_10)\n",
    "relu_21 = ReLU()(bn_21)\n",
    "drop_19 = Dropout(rate_drop)(relu_21)\n",
    "conv_22 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_19)\n",
    "bn_22 = BatchNormalization()(conv_22)\n",
    "relu_22 = ReLU()(bn_22)\n",
    "drop_20 = Dropout(rate_drop)(relu_22)\n",
    "conv_23 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_20)\n",
    "\n",
    "#Short connection\n",
    "short_11 = MaxPooling1D(pool_size = 1, strides=2)(add_10)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_11 = Add()([conv_23, short_11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo segundo bloco residual  \n",
    "Strides = 1  \n",
    "Filtros = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_23 = BatchNormalization()(add_11)\n",
    "relu_23 = ReLU()(bn_23)\n",
    "drop_21 = Dropout(rate_drop)(relu_23)\n",
    "conv_24 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_21)\n",
    "bn_24 = BatchNormalization()(conv_24)\n",
    "relu_24 = ReLU()(bn_24)\n",
    "drop_22 = Dropout(rate_drop)(relu_24)\n",
    "conv_25 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_22)\n",
    "\n",
    "#Short connection\n",
    "short_12 = MaxPooling1D(pool_size = 1, strides=1)(add_11)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_12 = Add()([conv_25, short_12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo terceiro bloco residual  \n",
    "Strides = 2  \n",
    "Filtros = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 256\n",
    "\n",
    "#Bloco residual\n",
    "bn_25 = BatchNormalization()(add_12)\n",
    "relu_25 = ReLU()(bn_25)\n",
    "drop_23 = Dropout(rate_drop)(relu_25)\n",
    "conv_26 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_23)\n",
    "bn_26 = BatchNormalization()(conv_26)\n",
    "relu_26 = ReLU()(bn_26)\n",
    "drop_24 = Dropout(rate_drop)(relu_26)\n",
    "conv_27 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_24)\n",
    "\n",
    "#Short connection\n",
    "conv_aj27 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\")(add_12)\n",
    "short_13 = MaxPooling1D(pool_size = 1, strides=2)(conv_aj27)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_13 = Add()([conv_27, short_13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo quarto bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_27 = BatchNormalization()(add_13)\n",
    "relu_27 = ReLU()(bn_27)\n",
    "drop_25 = Dropout(rate_drop)(relu_27)\n",
    "conv_28 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_25)\n",
    "bn_28 = BatchNormalization()(conv_28)\n",
    "relu_28 = ReLU()(bn_28)\n",
    "drop_26 = Dropout(rate_drop)(relu_28)\n",
    "conv_29 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_26)\n",
    "\n",
    "#Short connection\n",
    "short_14 = MaxPooling1D(pool_size = 1, strides=1)(add_13)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_14 = Add()([conv_29, short_14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo quinto bloco residual  \n",
    "Stride = 2  \n",
    "Filtros = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_29 = BatchNormalization()(add_14)\n",
    "relu_29 = ReLU()(bn_29)\n",
    "drop_27 = Dropout(rate_drop)(relu_29)\n",
    "conv_30 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_27)\n",
    "bn_30 = BatchNormalization()(conv_30)\n",
    "relu_30 = ReLU()(bn_30)\n",
    "drop_28 = Dropout(rate_drop)(relu_30)\n",
    "conv_31 = Conv1D(kernel_size=16, filters=num_filter, strides=2, padding=\"same\",kernel_initializer=initializer)(drop_28)\n",
    "\n",
    "#Short connection\n",
    "short_15 = MaxPooling1D(pool_size = 1, strides=2)(add_14)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_15 = Add()([conv_31, short_15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décimo sexto bloco residual  \n",
    "Stride = 1  \n",
    "Filtros = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloco residual\n",
    "bn_31 = BatchNormalization()(add_15)\n",
    "relu_31 = ReLU()(bn_31)\n",
    "drop_29 = Dropout(rate_drop)(relu_31)\n",
    "conv_32 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_29)\n",
    "bn_32 = BatchNormalization()(conv_32)\n",
    "relu_32 = ReLU()(bn_32)\n",
    "drop_30 = Dropout(rate_drop)(relu_32)\n",
    "conv_33 = Conv1D(kernel_size=16, filters=num_filter, strides=1, padding=\"same\",kernel_initializer=initializer)(drop_30)\n",
    "\n",
    "#Short connection\n",
    "short_16 = MaxPooling1D(pool_size = 1, strides=1)(add_15)\n",
    "\n",
    "#Adição dos blocos\n",
    "add_16 = Add()([conv_33, short_16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Último bloco da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Últimas camadas\n",
    "bn_33 = BatchNormalization()(add_16)\n",
    "relu_33 = ReLU()(bn_33)\n",
    "flat = Flatten()(relu_33)\n",
    "dense_1 = Dense(32)(flat)\n",
    "classification = Dense(5, activation='sigmoid')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando modelo\n",
    "model = Model(inputs=input_1,outputs=classification)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TREINANDO O MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parâmetros de otimização\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otimizador Adam\n",
    "opt = Adam(lr, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "#Reduz a taxa de aprendizagem quando o erro de validação para de melhorar\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=lr/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18358 samples, validate on 2055 samples\n",
      "Epoch 1/10\n",
      "18358/18358 [==============================] - 146s 8ms/sample - loss: 0.3885 - accuracy: 0.8315 - val_loss: 0.5447 - val_accuracy: 0.8484\n",
      "Epoch 2/10\n",
      "18358/18358 [==============================] - 108s 6ms/sample - loss: 0.3160 - accuracy: 0.8658 - val_loss: 0.3266 - val_accuracy: 0.8702\n",
      "Epoch 3/10\n",
      "18358/18358 [==============================] - 108s 6ms/sample - loss: 0.2969 - accuracy: 0.8753 - val_loss: 0.3176 - val_accuracy: 0.8672\n",
      "Epoch 4/10\n",
      "18358/18358 [==============================] - 108s 6ms/sample - loss: 0.2827 - accuracy: 0.8829 - val_loss: 0.4349 - val_accuracy: 0.8203\n",
      "Epoch 5/10\n",
      "18358/18358 [==============================] - 107s 6ms/sample - loss: 0.2741 - accuracy: 0.8864 - val_loss: 0.3105 - val_accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "18358/18358 [==============================] - 108s 6ms/sample - loss: 0.2692 - accuracy: 0.8886 - val_loss: 0.2998 - val_accuracy: 0.8808\n",
      "Epoch 7/10\n",
      "18358/18358 [==============================] - 108s 6ms/sample - loss: 0.2616 - accuracy: 0.8927 - val_loss: 0.2865 - val_accuracy: 0.8858\n",
      "Epoch 8/10\n",
      "18358/18358 [==============================] - 114s 6ms/sample - loss: 0.2559 - accuracy: 0.8960 - val_loss: 0.3591 - val_accuracy: 0.8678\n",
      "Epoch 9/10\n",
      "18358/18358 [==============================] - 117s 6ms/sample - loss: 0.2505 - accuracy: 0.8967 - val_loss: 0.2785 - val_accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "18358/18358 [==============================] - 116s 6ms/sample - loss: 0.2471 - accuracy: 0.8994 - val_loss: 0.2908 - val_accuracy: 0.8821\n"
     ]
    }
   ],
   "source": [
    "#Treino do modelo\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVALIANDO O MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "accuracy_train = history.history['accuracy']\n",
    "\n",
    "loss_val = history.history['val_loss']\n",
    "accuracy_val = history.history['val_accuracy']\n",
    "\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_train, '-o')\n",
    "plt.plot(epochs, loss_val, '-*')\n",
    "\n",
    "plt.title('Custo de treinamento e de validação')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Custo')\n",
    "plt.legend(['Custo de treino', 'Custo de validação'])\n",
    "local = 'E:/Usuários/Sarah/Documentos/UTFPR/TCC/Resultados/Gráficos/Custo_atual_100'\n",
    "plt.savefig(local)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy_train, '-o')\n",
    "plt.plot(epochs, accuracy_val, '-*')\n",
    "\n",
    "plt.title('Acurácia de treino e de validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend(['Acurácia de treino', 'Acurácia de validação'])\n",
    "local = 'E:/Usuários/Sarah/Documentos/UTFPR/TCC/Resultados/Gráficos/Acuracia_atual_100'\n",
    "plt.savefig(local)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print(f\"Custo de teste = {score[0]:.4f}\")\n",
    "print(f\"Acurácia de teste = {100*score[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predições**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification = ['NORM', 'MI', 'CD', 'STTC', 'HYP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test) #Realiza a predição das probabilidades de cada label\n",
    "prediction_bin = np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[0])\n",
    "print(prediction_bin[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma o vetor prediction que consta as probabilidades para cada label em um vetor binário (0 ou 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dada as probabilidades de cada label, se maior ou igual que 0.5, é considerada a classe de diagnóstico\n",
    "for indice in range(prediction_bin.shape[0]):\n",
    "    for i in range(prediction_bin.shape[1]):\n",
    "        if prediction_bin[indice][i] >= 0.5:\n",
    "            prediction_bin[indice][i] = 1\n",
    "        else:\n",
    "            prediction_bin[indice][i] = 0\n",
    "prediction_bin = np.array(prediction_bin,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Previsão: {prediction_bin[23]} \\t\\tLabel real: {y_test[23]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar o vetor binario em strings  \n",
    "https://medium.com/deep-learning-with-keras/how-to-solve-multi-label-classification-problems-in-deep-learning-with-tensorflow-keras-7fb933243595  \n",
    "https://machinelearningmastery.com/multi-label-classification-with-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_string = ['NORM','MI','CD','STTC','HYP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strings(label_string,label_bin):  #label_bin pode ser de teste ou predições\n",
    "    label_bin_string = []\n",
    "    for x in range(len(label_bin)):\n",
    "        lst = []\n",
    "        for y in range(len(label_string)):\n",
    "            value = label_bin[x][y]\n",
    "            if value == 1:\n",
    "                lst.append(label_string[y])\n",
    "        label_bin_string.append(lst)\n",
    "    \n",
    "    return label_bin_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando os vetores de label strings do vetor y_test\n",
    "y_test_string = get_strings(label_string,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando os vetores de label strings do vetor y_test\n",
    "prediction_string = get_strings(label_string,prediction_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_string[23])\n",
    "print(prediction_string[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando as predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 23\n",
    "print(f'Index: {index}\\n')\n",
    "\n",
    "print(f'Diagnóstico = {y_test[index]}')\n",
    "print(f'Predição = {prediction_bin[index]}\\n')\n",
    "\n",
    "print(f'Diagnóstico = {y_test_string[index]}')\n",
    "print(f'Predição = {prediction_string[index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar a acc do exemplo correspondente ao índice!\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_index = accuracy_score(y_test[index],prediction_bin[index])\n",
    "print(f'Acurácia do exemplo = {100 * acc_index:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_med = X_test[index, ].mean(axis=-1)\n",
    "fig_s, ax_s = plt.subplots(figsize=(10,7))\n",
    "ax_s.set_title(f'Diagnóstico: {y_test_string[index]}       Predição:{prediction_string[index]}')\n",
    "ax_s.plot(valor_med)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**  \n",
    "Precision can be seen as a measure of a classifier’s exactness. For each class, it is defined as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”  \n",
    "\n",
    "**Recall**  \n",
    "Recall is a measure of the classifier’s completeness; the ability of a classifier to correctly find all positive instances. For each class, it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”  \n",
    "\n",
    "**F1 score**  \n",
    "The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.  \n",
    "\n",
    "**Support**  \n",
    "Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process.  \n",
    "\n",
    "https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html#:~:text=not%20global%20accuracy.-,support,for%20stratified%20sampling%20or%20rebalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test,prediction_bin,output_dict=True,target_names=['NORM', 'MI', 'CD', 'STTC', 'HYP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame.from_dict(report, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = multilabel_confusion_matrix(y_test, prediction_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/trolukovich/multi-label-classification-keras/notebook\n",
    "# Plot confusion matrix \n",
    "fig = plt.figure(figsize = (14, 8))\n",
    "for i, (label, matrix) in enumerate(zip(label_string, cm)):\n",
    "    plt.subplot(f'23{i+1}')\n",
    "    labels = [f'Não {label}', label]\n",
    "    sns.heatmap(matrix, annot = True, square = True, fmt = 'd', cbar = False, cmap = 'Blues', \n",
    "                xticklabels = labels, yticklabels = labels, linecolor = 'black', linewidth = 1)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
